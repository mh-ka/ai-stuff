# Audio

## speech2txt

### OpenAI's Whisper
- [Github](https://github.com/openai/whisper)
- [Distil-Whisper](https://github.com/huggingface/distil-whisper/issues/4)
- [Insanely fast whisper](https://github.com/Vaibhavs10/insanely-fast-whisper)
- [WhisperKit for Apple devices](https://www.takeargmax.com/blog/whisperkit)
- [Whisper turbo](https://github.com/openai/whisper/discussions/2363)
- [Whisper Medusa](https://github.com/aiola-lab/whisper-medusa)
- [Tips against hallucinations](https://www.reddit.com/r/LocalLLaMA/comments/1fx7ri8/comment/lql41mk/)
- [Whisper Standalone Win](https://github.com/Purfview/whisper-standalone-win)
- [Whisperfile](https://github.com/Mozilla-Ocho/llamafile/blob/main/whisper.cpp/doc/getting-started.md)
- [WhisperX](https://github.com/m-bain/whisperX)
- [ctranslate2](https://github.com/Softcatala/whisper-ctranslate2)
- [Whisper WebUI](https://github.com/jhj0517/Whisper-WebUI)
- [MisterWhisper](https://github.com/openconcerto/MisterWhisper)
- [Easy Whisper UI](https://github.com/mehtabmahir/easy-whisper-ui)
- [SoftWhisper](https://github.com/NullMagic2/SoftWhisper)
- [Whisper-diarization](https://github.com/MahmoudAshraf97/whisper-diarization)

### Nvidia
- Canary (with translation)
  - https://nvidia.github.io/NeMo/blogs/2024/2024-02-canary/
  - https://huggingface.co/nvidia/canary-1b-flash
- Parakeet
  - https://github.com/Shadowfita/parakeet-tdt-0.6b-v2-fastapi
  - https://github.com/SridharSampath/parakeet-asr-demo

### Other
- [Qwen2-Audio-7B](https://huggingface.co/Qwen/Qwen2-Audio-7B)
- [Speech2Speech pipeline](https://github.com/huggingface/speech-to-speech)
- [Moonshine](https://github.com/usefulsensors/moonshine)
- [Article about Speech recognition (comparisons and insights)](https://amgadhasan.substack.com/p/sota-asr-tooling-long-form-transcription)
- [DeepFilter for filtering noisy audio](https://github.com/duohub-ai/deepfilter-lambda-container)
- [Vosk](https://alphacephei.com/vosk/)
- [MeetingBuddy (GUI for realtime transcription)](https://github.com/psdwizzard/MeetingBuddy)
- [HalfwayML (OSS transcription service)](https://github.com/moaljumaa/halfwayml_open)
- [Vibe (GUI for transcription)](https://github.com/thewh1teagle/vibe/)
- [Real Time Speech Transcription FastRTC](https://github.com/sofi444/realtime-transcription-fastrtc)
- [IBM Granite Speech](https://huggingface.co/collections/ibm-granite/granite-speech-67e45da088d5092ff6b901c7)
- [STT streaming](https://github.com/kyutai-labs/delayed-streams-modeling)
- [Transcription App (Rust based)](https://github.com/cjpais/Handy)


## txt2speech and txt2audio

### Fishaudio
- [Fish Speech 1.4](https://huggingface.co/fishaudio/fish-speech-1.4)
- [Fish Speech 1.5](https://huggingface.co/fishaudio/fish-speech-1.5)

### XTTS
- [XTTS v1](https://huggingface.co/coqui/XTTS-v1)
- [XTTS v2](https://huggingface.co/coqui/XTTS-v2)

### Kokoro
- [Model](https://huggingface.co/hexgrad/Kokoro-82M)
- [ONNX variant](https://huggingface.co/onnx-community/Kokoro-82M-ONNX)
- [Dockerized](https://github.com/remsky/Kokoro-FastAPI)
- [Kokoros (Rust based engine)](https://github.com/lucasjinreal/Kokoros)
- [KokoDOS (GlaDOS fork)](https://github.com/kaminoer/KokoDOS)
- [kokoro-js](https://www.npmjs.com/package/kokoro-js)
- [KokoVoiceLabs](https://github.com/RobViren/kokovoicelab)
- [Kokoro-Web](https://github.com/xenova/kokoro-web)

### OuteTTS
- [v0.2 (onnx model for transformer.js WebGPU inference)](https://huggingface.co/onnx-community/OuteTTS-0.2-500M)
- [v0.3](https://huggingface.co/collections/OuteAI/outetts-03-6786b1ebc7aeb757bc17a2fa)
- [v1.0](https://huggingface.co/collections/OuteAI/outetts-10-67f3f4137df2e411b1bab852)

### Zonos
- [v0.1](https://huggingface.co/Zyphra/Zonos-v0.1-transformer)
- [Github](https://github.com/Zyphra/Zonos)
- [Windows fork](https://github.com/sdbds/Zonos-for-windows)
- [Core Zonos](https://github.com/loscrossos/core_zonos)

### StepAudio
- [TTS 3B](https://huggingface.co/stepfun-ai/Step-Audio-TTS-3B)

### Orpheus-TTS
- [Orpheus-TTS](https://github.com/canopyai/Orpheus-TTS)
- [Local WebUI](https://github.com/akashjss/orpheus-tts-local-webui)
- [3B Fine-tune](https://huggingface.co/canopylabs/orpheus-3b-0.1-ft)

### Chatterbox
- [Huggingface](https://huggingface.co/ResembleAI/chatterbox)
- [ComfyUI-Fill-Chatterbox](https://github.com/filliptm/ComfyUI_Fill-ChatterBox)
- [Chatterbox-TTS-Extended](https://github.com/petermg/Chatterbox-TTS-Extended)
- [ChatterboxToolkitUI](https://github.com/dasjoms/ChatterboxToolkitUI)
- [Audiobook](https://github.com/psdwizzard/chatterbox-Audiobook)

### Dia
- [Dia 1.6B](https://github.com/nari-labs/dia)
- [NotebookLM style fork](https://github.com/PasiKoodaa/dia)
- [Dia Podcast Generator](https://github.com/smartaces/dia_podcast_generator)

### Other
- Models
  - [AudioLDM2](https://github.com/haoheliu/audioldm2)
  - [VoiceCraft](https://github.com/jasonppy/VoiceCraft)
  - [Bark](https://github.com/suno-ai/bark)
  - [MetaVoice](https://github.com/metavoiceio/metavoice-src)
  - [MARS5-TTS](https://github.com/Camb-ai/MARS5-TTS)
  - [Alibaba's FunAudioLLM framework (includes CosyVoice & SenseVoice)](https://github.com/FunAudioLLM)
  - [MeloTTS](https://github.com/myshell-ai/MeloTTS)
  - [Parler TTS](https://github.com/huggingface/parler-tts)
  - [WhisperSpeech](https://github.com/collabora/WhisperSpeech)
  - [ChatTTS](https://huggingface.co/2Noise/ChatTTS)
  - [GPT-SoVITS-WebUI](https://github.com/RVC-Boss/GPT-SoVITS)
  - [F5 TTS](https://github.com/SWivid/F5-TTS)
  - [MaskGCT](https://huggingface.co/amphion/MaskGCT)
  - German TTS model
    - [Thorsten voice](https://github.com/thorstenMueller/Thorsten-Voice)
    - [German TTS on Huggingface](https://huggingface.co/models?search=German%20tts)
  - Llasa (TTS and voice cloning)
    - [Github setup](https://github.com/nivibilla/local-llasa-tts)
    - [Huggingface model collection](https://huggingface.co/collections/HKUSTAudio/llasa-679b87dbd06ac556cc0e0f44)
    - [Blog page](https://huggingface.co/blog/srinivasbilla/llasa-tts)
  - [Spark-TTS (LLM-based TTS)](https://github.com/SparkAudio/Spark-TTS)
  - [Index-TTS](https://github.com/index-tts/index-tts)
- Audiobook creation
  - [ebook2audiobook](https://github.com/DrewThomasson/ebook2audiobookXTTS)
  - [autiobooks (audiobook generation)](https://github.com/plusuncold/autiobooks)
  - [Audiobook creator](https://github.com/prakharsr/audiobook-creator/)
  - [Audiblez (ebook2audiobook)](https://github.com/santinic/audiblez)
  - [Audiobook generator](https://github.com/houtianze/audiobook-generator)
  - [Audiobook chatterbox](https://github.com/psdwizzard/chatterbox-Audiobook)
- Infos
  - [Tracker page for open access text2speech models](https://github.com/Vaibhavs10/open-tts-tracker)
  - [TTS Arena Leaderboard](https://huggingface.co/spaces/TTS-AGI/TTS-Arena)
  - [Example script for text to voice](https://github.com/dynamiccreator/voice-text-reader)
  - [Pheme TTS framework](https://github.com/PolyAI-LDN/pheme)
  - [OpenAI TTS](https://platform.openai.com/docs/guides/text-to-speech)
  - [OpenVoice](https://github.com/myshell-ai/OpenVoice)
- text2audio and more
  - [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0)
  - [Audiocraft Plus](https://github.com/GrandaddyShmax/audiocraft_plus)
  - [AudioX](https://github.com/ZeyueT/AudioX)
- [TTS server](https://github.com/matatonic/openedai-speech)
- [Piper (local TTS system)](https://github.com/rhasspy/piper)
- [Voqal (TTS AI agent)](https://github.com/voqal/voqal)
- [TTS library](https://github.com/idiap/coqui-ai-TTS)
- [Sherpa-Onnx (framework for STT, TTS, etc.)](https://github.com/k2-fsa/sherpa-onnx)
- [Speaches (server for STT, translation, TTS)](https://github.com/speaches-ai/speaches)
- [OpenReader (document reader)](https://github.com/richardr1126/OpenReader-WebUI)
- [OpenDubbing](https://github.com/Softcatala/open-dubbing)
- [Voice datasets](https://github.com/jim-schwoebel/voice_datasets)
- [TTS in C#](https://github.com/DillionLowry/NeuralCodecs)


## Music production and generation

### Production
- [LANDR mastering plugin](https://www.gearnews.de/landr-mastering-plugin/)
- [Drumloop.ai](https://www.gearnews.de/drumloop-ai-baut-euch-automatisch-beats-und-drumloops-durch-ki/)
- [Sample generator](https://huggingface.co/adlb/Audialab_EDM_Elements)
- [RC stable audio tools (Gradio app for using audio models)](https://github.com/RoyalCities/RC-stable-audio-tools)
- stem-creators
  - [Voice_Extractor](https://github.com/ReisCook/Voice_Extractor)
  - [Ultimate Vocal Remover GUI](https://github.com/Anjok07/ultimatevocalremovergui)
  - [Demucs](https://github.com/adefossez/demucs)

### Generation
- [YuE GP](https://github.com/deepbeepmeep/YuEGP)
- [DiffRhythm](https://github.com/ASLP-lab/DiffRhythm)
- ACE-Step
  - [ACE-Step](https://github.com/ace-step/ACE-Step)
  - [ACE-Step-v1-3.5B](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)
  - [ComfyUI_ACE-Step](https://github.com/billwuhao/ComfyUI_ACE-Step)
  - [ACE-Step-Radio](https://github.com/PasiKoodaa/ACE-Step-RADIO)
- [Magenta Realtime](https://github.com/magenta/magenta-realtime)


## Other audio models and related tools and resources
- [LAION AI Voice Assistant BUD-E](https://github.com/LAION-AI/natural_voice_assistant)
- AI Language Tutor
  - https://www.univerbal.app/
  - https://yourteacher.ai/
- [Speech Note Offline STT, TTS and Machine Translation](https://github.com/mkiol/dsnote)
- [DenseAV (locates sound and learns meaning of words)](https://github.com/mhamilton723/DenseAV)
- [Moshi (speech2speech foundation model)](https://huggingface.co/collections/kyutai/moshi-v01-release-66eaeaf3302bef6bd9ad7acd)
- [Open VTuber App](https://github.com/t41372/Open-LLM-VTuber)
- [Voicechat implementation](https://github.com/lhl/voicechat2)
- [Podcastfy](https://github.com/souzatharsis/podcastfy)
- [Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)
- [Ebook2audiobook](https://github.com/DrewThomasson/ebook2audiobookpiper-tts)
- [Voice Conversion](https://github.com/IAHispano/Applio)
- [TTS comparison](https://tts.x86.st/)
- [Voice cloning tutorial](https://techshinobi.org/posts/voice-vits/)
- [GlaDOS](https://github.com/dnhkng/GlaDOS)
- [ClearerVoice-Studio](https://github.com/modelscope/ClearerVoice-Studio/tree/main)
- [OmniAudio 2.6B (edge device setup for taking input audio and integrate LLM)](https://huggingface.co/NexaAIDev/OmniAudio-2.6B)
- [BlahST (speech2txt tool based on whisper for linux)](https://github.com/QuantiusBenignus/BlahST)
- [Weebo (speech-to-speech chatbot using whisper, llama, kokoro)](https://github.com/amanvirparhar/weebo)
- [AI voice agents overview](https://a16z.com/ai-voice-agents-2025-update)
- [Hibiki (Speech2Speech translation en <-> fr)](https://github.com/kyutai-labs/hibiki)
- [Sage (speech conversation with LLMs)](https://github.com/farshed/sage)
- [chaplin (lip read / visual speech recognition)](https://github.com/amanvirparhar/chaplin)
- [Meta's SeamlessM4T (translation framework)](https://huggingface.co/facebook/seamless-m4t-v2-large)
- [MiraConverse (voice interaction with LLMs)](https://github.com/KartDriver/mira_converse)
- [OllamaGTTS](https://github.com/ExoFi-Labs/OllamaGTTS)
- [Vocal-Agent](https://github.com/tarun7r/Vocal-Agent)
- [Vocalis](https://github.com/Lex-au/Vocalis)
- [Persona Engine](https://github.com/fagenorn/handcrafted-persona-engine)
- [RealtimeVoiceChat](https://github.com/KoljaB/RealtimeVoiceChat)
- [Kimi-Audio (universal audio model)](https://github.com/MoonshotAI/Kimi-Audio)
- [PlayDiffusion (audio inpainting)](https://github.com/playht/PlayDiffusion)
- [MMAudio (video-to-audio)](https://github.com/hkchengrex/MMAudio)
